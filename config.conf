# Server setup config
USE_GPU=false
LLAMA_CPP_VERSION_TAG=b3829                                  # https://github.com/ggerganov/llama.cpp/releases
LLAMA_PORT=6371

# Server runtime config
MODEL_FILE=gemma-1.1-7b-it.Q4_K_M.gguf                       # high-end server recommendation: Meta-Llama-3.1-70B-Instruct-Q5_K_S.gguf
N_PREDICT=180                                                # max number of tokens to predict
TEMP=0                                                       # temperature for predictions
NGL=99                                                       # number of layers loaded to gpu (only relevant if USE_GPU=true). If cudaMalloc fails during server startup, this value is most likely too large.
TIMEOUT=30                                                   # network read/write timeout, does not effect the inference time

# Client config
SERVER_ADDRESS=[0000:0000:0000:0000:0000:0000:0000:0001]     # default value is localhost (IPv6). Relevant only for the client script to connect to the server. To find the public IPv6 on the server machine, use: curl -6 https://ifconfig.co

# Misc
HUGGING_FACE_MODEL_REPO=ggml-org/gemma-1.1-7b-it-Q4_K_M-GGUF # high-end server recommendation: Meta-Llama-3.1-70B-Instruct-Q5_K_S.gguf. This configuration variable is relevant only if the model file is downloaded by download_model.sh
NUMBER_OF_COMPILE_THREADS=8
